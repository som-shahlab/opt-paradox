# ClinAgents Configuration File (template)
# =============================================================================
# Copy this to `config.yaml` and fill in your values.
#
# 1. Paste your API keys in “api_keys.”
# 2. Point “paths.dataset_base_path” at your data directory.
# 3. Under “endpoints,” replace the `api_base` placeholder with your endpoint URL.
#
# After editing:
#   python -m src.main --config src/config.yaml
# =============================================================================

# -----------------------------------------------------------------------------
# 1. API Keys
# -----------------------------------------------------------------------------
api_keys:
  azure_key:  "YOUR_AZURE_KEY"

# -----------------------------------------------------------------------------
# 2. Data Paths
# -----------------------------------------------------------------------------
paths:
  dataset_base_path: "/path/to/directory/containing/clinagents/data"

# -----------------------------------------------------------------------------
# 3. Azure LLM Endpoints & Deployments
# -----------------------------------------------------------------------------
# Replace `api_base` with your actual endpoint URL; keep model_id/api_version.
endpoints:
  gpt:
    api_base:    "https://YOUR_API_BASE_URL"
    model_id:    "gpt-4o"
    api_version: "2023-05-15"

  gpt41:
    api_base:    "https://YOUR_API_BASE_URL"
    model_id:    "gpt-4.1"
    api_version: "2025-01-01-preview"

  gpt41mini:
    api_base:    "https://YOUR_API_BASE_URL"
    model_id:    "gpt-4.1-mini"
    api_version: "2025-01-01-preview"

  claude:
    api_base:    "https://YOUR_API_BASE_URL"
    model_id:    "anthropic.claude-3-5-sonnet-20241022-v2:0"

  gemini:
    api_base:    "https://YOUR_API_BASE_URL"
    model_id:    "gemini-1.5-pro"

  gemini_flash:
    api_base:    "https://YOUR_API_BASE_URL"
    model_id:    "gemini-2.0-flash"

  llama:
    api_base:    "https://YOUR_API_BASE_URL"
    model_id:    "Llama-3.3-70B-Instruct"

  o3_mini:
    api_base:    "https://YOUR_API_BASE_URL"
    model_id:    "o3-mini"
    api_version: "2024-12-01-preview"

  deepseek_r1:
    api_base:    "https://YOUR_API_BASE_URL"
    model_id:    "deepseek-chat"

# -----------------------------------------------------------------------------
# 4. Model-Loading Defaults
# -----------------------------------------------------------------------------
model_loading:
  default_max_tokens:  32000
  matcher_max_tokens:   800
  default_temperature:  0.1

# -----------------------------------------------------------------------------
# 5. Cost Tracking (per token)
# -----------------------------------------------------------------------------
cost_tracking:
  cost_table:
    claude-3-5-sonnet:    { input: 0.00000375, output: 0.00001500 }
    gemini-1.5-pro:       { input: 0.00000125, output: 0.00000500 }
    gemini-2.0-flash:     { input: 0.00000010, output: 0.00000040 }
    gpt-4o:               { input: 0.00000250, output: 0.00001000 }
    gpt-4.1:              { input: 0.00000200, output: 0.00000800 }
    o3-mini:              { input: 0.00000110, output: 0.00000440 }
    llama-3.3-70b:        { input: 0.00000071, output: 0.00000071 }
    deepseek-chat:        { input: 0.00000050, output: 0.00000050 }

  model_cost_mapping:
    claude:        "claude-3-5-sonnet"
    gemini:        "gemini-1.5-pro"
    gemini-flash:  "gemini-2.0-flash"
    gpt:           "gpt-4o"
    gpt-4.1:       "gpt-4.1"
    llama:         "llama-3-70b-instruct"
    o3-mini:       "o3-mini"
    deepseek:      "deepseek-chat"

# -----------------------------------------------------------------------------
# 6. Runtime Parameters
# -----------------------------------------------------------------------------
runtime:
  max_iterations:                10
  print_every:                   10
  gc_every:                      50
  log_to_file:                   false
